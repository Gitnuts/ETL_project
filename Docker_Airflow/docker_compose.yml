version: '3.7'

services:
  postgres:
    image: postgres:9.6
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  airflow:
    image: puckel/docker-airflow:1.10.1
    build:
      context: https://github.com/puckel/docker-airflow.git#1.10.1
      dockerfile: Dockerfile
      args:
        AIRFLOW_DEPS: gcp_api,s3
        PYTHON_DEPS: sqlalchemy==1.2.0
    restart: always
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
    volumes:
      - ./examples/intro-example/dags:/usr/local/airflow/dags
      - ./airflow_docker/requirements.txt:/requirements.txt
      # Uncomment to include custom plugins
      # - ./plugins:/usr/local/airflow/plugins
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

  zookeeper:                                          # create zookeeper container
    image: wurstmeister/zookeeper
    container_name: zookeeper_container
    ports:
        - "2181:2181"

  kafka:                                              # create an instance of a Kafka broker in a container
    image: wurstmeister/kafka
    container_name: kafka_container
    ports:
        - "9092:9092"                               # expose port
    environment:
        KAFKA_ADVERTISED_HOST_NAME: kafka                               # specify the docker host IP at which other containers can reach the broker
        KAFKA_CREATE_TOPICS: "test:1:1"           # create a 2 topics  with 1 partition and 1 replica
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181                         # specify where the broker can reach Zookeeper
        KAFKA_LISTENERS: PLAINTEXT://kafka:9092                         # the list of addresses on which the Kafka broker will listen on for incoming connections.
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092              # Kafka sends the value of this variable to clients during their connection. After receiving that value, the clients use it for sending/consuming records to/from the Kafka broker.y connect to it.
    volumes:
        - /var/run/docker.sock:/var/run/docker.sock

  spark:                                             # create a spark container
    build: './spark_hadoop_docker'                 # construct the container along the Dockerfile in this folder
    container_name: spark_hadoop_container
    ports:
        - "5000:5000"                               # expose port
    tty: true             
